# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DCka7ILjFOOhFhIscBWHZuWzFid6CZiH
"""

import openai
import os
import uuid
from pinecone import Pinecone
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
import streamlit as st

os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
os.environ["PINECONE_API_KEY"] = st.secrets["PINECONE_API_KEY"]
os.environ["INDEX_HOST"] = st.secrets["INDEX_HOST"]

# textmodels/ constants
TEXT_MODEL = "text-embedding-ada-002"
namespace= "oceansafe"
COMMON_TEMPLATE = """
"Use the following pieces of context to answer the question at the end with human readable answer as a paragraph"
"Please do not use data outside the context to answer any questions. "
"If the answer is not in the given context, just say that you don't have enough context."
"don't try to make up an answer. "
"\n\n"
{context}
"\n\n"
Question: {question}
"n"
"Helpful answer:   "
"""

# pinecone setups
client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"])
pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"])
index = pc.Index(host=os.environ["INDEX_HOST"])

# Functions

def retrive_embed_openai(txt):
  response = client.embeddings.create(input=f"{txt}", model=TEXT_MODEL)

  return response.data[0].embedding

def retrivesimilair(query_embed, k=2 , namspace = "oceansafe" ):
  query_response = index.query(
        namespace = namespace,
        vector = query_embed,
        top_k = k,
        include_values=False,
        include_metadata=True,
    )
  return query_response

def content_extraction(query_response):
  top_chunks = query_response["matches"]
  text_content = [sub_content["metadata"]["text"] for sub_content in top_chunks]
  return " ".join(text_content)

def query_answering(query_embed, context, template = COMMON_TEMPLATE):
  prompt = ChatPromptTemplate.from_template(template)
  model = ChatOpenAI(model="gpt-4o-mini", api_key=os.environ["OPENAI_API_KEY"])
  output_parser = StrOutputParser()
  chain = prompt| model| output_parser
  answer = chain.invoke({"context": context, "question": query_embed})
  return answer

def response_generator(query):
  query_embed = retrive_embed_openai(query)
  query_similair = retrivesimilair(query_embed)
  query_extract = content_extraction(query_similair)
  query_ans = query_answering(query, query_extract)
  return query_ans

